# TensorRT, Triton

Экспорт модели в ONNX, работа с Nvidia Triton Inference Server, компиляция в TensorRT

Запускаем скрипт torch2onnx.py для конвертации в onnx

Запускаем скрипт convert_to_tensorrt.sh для конвертации в tensorrt

Дальше надо поднять контейнер и прогнать на нем наш сервер

